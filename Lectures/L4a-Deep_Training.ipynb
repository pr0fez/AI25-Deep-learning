{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6722750e",
   "metadata": {},
   "source": [
    "## Gradienter, backpropagation\n",
    "\n",
    "Gradienterna i djupa nätverk är inte stabila vid numerisk derivering (beräkning av gradienterna). Två huvudsakliga beteenden observeras:\n",
    "* Försvinnande gradient (Vanishing Gradient) -- gradienten går mot noll desto djupare i närverket algoritmen går. Alltså slutar nätverket lära sig och de djupa lagren slutar uppdateras. Nätverket konvergerar inte mot någon lösning.\n",
    "* Exploderande gradient (Exploding Gradient) -- gradienten divergerar och därmed även den förutsagda lösningen. Dessa gradienter kan till och med vara periodiska eller kaotiska. Förekommer i allmänhet bara i _rekurrenta_ nätverk (RNN), även om undantag finns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c92d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Vad som händer matematiskt\n",
    "\n",
    "Detta var, som så mycket annat, en empirisk upptäckt. Ingen matematik fanns som förklarade beteendet\n",
    "och ledde till att DNN (djupa neurala nätverk) övergavs kring 2000. Xavier Glorot och Yoshua Bengio \n",
    "visade kring 2010 att kombinationen av den aktiveringsfunktion som var mest populär (sigmoid) och \n",
    "hur vikterna i nätverket initialiserades ledde till att variansen ökade i varje lager. Det var alltså\n",
    "en kombination av en statistisk effekt och ett numerisk närmevärde.\n",
    "\n",
    "<img src=\"../Data/sigmoid_sat.jpg\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf199f2d",
   "metadata": {},
   "source": [
    "I allmänhet kan inte variansen för både gradienterna och grundsanningen hållas nere om inte alla lager har samma antal kopplingar in och ut, vilket är en ytterligare anledning varför vissa nätverk föredrar att hålla samma antal noder i alla lager. Men Glorot och Bengio hittade en bra kompromiss, som visar sig fungera bra nog i praktiken: så-kallad _Glorot initialisering_.\n",
    "\n",
    "Men den tekniken gäller bara för sigmoida funktioner. För ReLU föredras _He-initialisering_ (även _Kaiming-initialisering_ efter forskaren Kaiming He). Dessa skiljer sig mest i skalfaktorer och vilken statistika som används för variansen (in, ut eller medel). En till vanlig initalisering är _LeCun_ initialisering, som oftast används med SELU men även just nätverk där antalet noder i alla lager är detsamma.\n",
    "\n",
    "<img src=\"../Data/active_init.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8fc19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# av historiska skäl fungerar initialisering lite konstigt i Torch\n",
    "# bäst är att göra det explicit:\n",
    "\n",
    "layer = nn.Linear(40,10)\n",
    "\n",
    "## He-initialisation\n",
    "nn.init.kaiming_uniform_(layer.weight)\n",
    "nn.init.zeros_(layer.bias) # bias kan lika gärna börja på noll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf6d8c",
   "metadata": {},
   "source": [
    "### Aktiveringsfunktioner\n",
    "\n",
    "Så vilken aktiveringsfunktion skall man välja?\n",
    "\n",
    "I praktiken är ReLU väldigt snabb att beräkna och mättas inte för positiva värden (har nollskild gradient), men den har ett problem som kallas \"döende ReLU\" -- under träningen kan dessa aktiveringsfunktioner hamna i ett läga då de alltid får negativ input och alltså alltid ger tillbaka 0. Därefter är den noden permanent 0, och alltså \"död\". \n",
    "\n",
    "En lösning är LeakyReLU:\n",
    "\n",
    "<img src=\"../Data/LeakReLU.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458318bb",
   "metadata": {},
   "source": [
    "Även en variant där vinkeln är en parameter, PReLU, finns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98d7965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3284,  0.2172,  0.2044,  ..., -0.1514,  0.3106, -0.2085],\n",
       "        [ 0.2626, -0.1736, -0.1652,  ...,  0.0633,  0.2117, -0.2278],\n",
       "        [ 0.2743,  0.2329,  0.0864,  ..., -0.2240,  0.2207, -0.1044],\n",
       "        ...,\n",
       "        [-0.1555, -0.2045, -0.0535,  ..., -0.0955,  0.2273, -0.3180],\n",
       "        [ 0.1182,  0.1127,  0.0417,  ...,  0.2725, -0.0416, -0.0705],\n",
       "        [ 0.3118, -0.0245,  0.3074,  ..., -0.0928, -0.1516,  0.3248]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(50, 40),\n",
    "    nn.LeakyReLU(negative_slope=alpha)    \n",
    ")\n",
    "nn.init.kaiming_uniform_(model[0].weight, alpha, nonlinearity=\"leaky_relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368a251",
   "metadata": {},
   "source": [
    "#### GELU, Swish, SwiGLU, Mish, RELU²\n",
    "\n",
    "<img src=\"../Data/GELU.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38c34f",
   "metadata": {},
   "source": [
    "GELU, Mish och Swish finns som <code>nn.GELU</code>, <code>nn.Mish</code> och <code>nn.SiLU</code>\n",
    "\n",
    "Se boken kapitel 11 för detaljer och hur man implementerar de andra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179ff95",
   "metadata": {},
   "source": [
    "Swish och ReLU i praktiken vanligast, men tanh är viktig för LLMer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e1412",
   "metadata": {},
   "source": [
    "## Normalisering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e6a8b",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "Initialisering hjälper bara i början av träningen och inte hindrar inte döda noder från att dyka upp senare i träningen. En teknik för att åtgärda detta är normalisering. Eftersom vi har att göra med neurala nätverk så preprocessar vi inte datan med tex StandardScaler utan lägger helt enkelt till ett lager i nätverket som utför normaliseringen! Ett typiskt sådant lager är ett _Batch Normalization Layer_ -- ett BN-lager. Notera att detta bara opererar på varje _batch_, dvs stickprov, och inte på hela träningsdatan. Vi måste därför vara noggranna med att blanda \n",
    "datan så att vi undviker konstiga medelvärden i stickproven. Dessutom behöver vi hålla reda på ett glidande medelvärde för att ha någon chans att fånga populationsmedlet under evaluering! Om vi inte gör det kommer skaleringen bli helt tokig när vi använder nätverket. Pytorch håller reda på detta åt oss, men nu börjar det bli viktigt att använda `model.train()` och `model.eval()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d210df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.BatchNorm1d(1*28*28), # bilderna i mnist fashion är 28x28, vi plattade till ovanför\n",
    "    nn.Linear(1 * 28 * 28, 300),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(300),\n",
    "    nn.Linear(300, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.Linear(100, 10)\n",
    ")\n",
    "for layer in mnist_model:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "\n",
    "# Detta kommer inte ha så stor effekt -- det är relevant i mycket djupare nätverk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a614c5",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "\n",
    "Istället för att normalisera över stickprovets dimension (dvs värdena) så normaliserar lagernormalisering över feature-dimensionen. Det kan tyckas förvånande att det fungerar, men \n",
    "då skall vi minnas att vi skalerar och normaliserar just för att features skall vara i liknande\n",
    "skala. Till skilland från BN behöver LN inte hålla reda på glidande medel, utan lär sig distributionen under träningen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235fc92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 100, 200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(32, 3, 100, 200) # ett stickprov om 32 st 3-färgers 100x200 slumpade bilder\n",
    "layer_norm = nn.LayerNorm([3, 100, 200])\n",
    "result = layer_norm(inputs) # normaliserar över alla tre färgkanaler samtidigt\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac9b38",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "En kraftfull teknik, som dock har ett visst pris när det gäller statistik under träningen, är att sätta en sannolikhet för att en nod skall vara med eller inte under träningen. Detta betyder i praktiken att varje träningspass sker på ett nytt nätverk, med andra kopplingar. Det innebär att val/train accuracy och loss inte längre är så betydelsefulla, utan vi måste använda separata tester på okänd data. \n",
    "\n",
    "Extra viktigt att komma ihåg att byta till `model.eval()` innan nätverket används -- annars blir det något slumpässigt del-nätverk som kör varje gång!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(p=0.2), \n",
    "    nn.Linear(1 * 28 * 28, 100), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2), \n",
    "    nn.Linear(100, 100), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2), \n",
    "    nn.Linear(100, 100), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2), \n",
    "    nn.Linear(100, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a77e0",
   "metadata": {},
   "source": [
    "Se kapitel 11 för än annu bättre version, som använder Monte Carlo simulering för att bestämma dropout!\n",
    "\n",
    "<img src=\"../Data/perf_guidelines.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec973f4",
   "metadata": {},
   "source": [
    "Weight-decay är ungefär lika med $\\mathcal{l}_2$ regularisering och är en hyper-parameter på `AdamW` optimeraren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e8945",
   "metadata": {},
   "source": [
    "## Visualisering\n",
    "\n",
    "[Cats vs Dogs](../Resources/keras/L4b.dogs_vs_cats.ipynb)\n",
    "\n",
    "[Visualising cats vs dogs](../Resources/keras/L4c.visualizing_cnns.ipynb)\n",
    "\n",
    "[Feature visualization with deep dream-like optimization](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "[Extension of the above, exploration of InceptionV1 (aka GoogLeNet)](https://distill.pub/2019/activation-atlas/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
