{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6722750e",
   "metadata": {},
   "source": [
    "## Gradienter, backpropagation\n",
    "\n",
    "Gradienterna i djupa nätverk är inte stabila vid numerisk derivering (beräkning av gradienterna). Två huvudsakliga beteenden observeras:\n",
    "* Försvinnande gradient (Vanishing Gradient) -- gradienten går mot noll desto djupare i närverket algoritmen går. Alltså slutar nätverket lära sig och de djupa lagren slutar uppdateras. Nätverket konvergerar inte mot någon lösning.\n",
    "* Exploderande gradient (Exploding Gradient) -- gradienten divergerar och därmed även den förutsagda lösningen. Dessa gradienter kan till och med vara periodiska eller kaotiska. Förekommer i allmänhet bara i _rekurrenta_ nätverk (RNN), även om undantag finns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c92d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Vad som händer matematiskt\n",
    "\n",
    "Detta var, som så mycket annat, en empirisk upptäckt. Ingen matematik fanns som förklarade beteendet\n",
    "och ledde till att DNN (djupa neurala nätverk) övergavs kring 2000. Xavier Glorot och Yoshua Bengio \n",
    "visade kring 2010 att kombinationen av den aktiveringsfunktion som var mest populär (sigmoid) och \n",
    "hur vikterna i nätverket initialiserades ledde till att variansen ökade i varje lager. Det var alltså\n",
    "en kombination av en statistisk effekt och ett numerisk närmevärde.\n",
    "\n",
    "<img src=\"../Data/sigmoid_sat.jpg\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf199f2d",
   "metadata": {},
   "source": [
    "I allmänhet kan inte variansen för både gradienterna och grundsanningen hållas nere om inte alla lager har samma antal kopplingar in och ut, vilket är en ytterligare anledning varför vissa nätverk föredrar att hålla samma antal noder i alla lager. Men Glorot och Bengio hittade en bra kompromiss, som visar sig fungera bra nog i praktiken: så-kallad _Glorot initialisering_.\n",
    "\n",
    "Men den tekniken gäller bara för sigmoida funktioner. För ReLU föredras _He-initialisering_ (även _Kaiming-initialisering_ efter forskaren Kaiming He). Dessa skiljer sig mest i skalfaktorer och vilken statistika som används för variansen (in, ut eller medel). En till vanlig initalisering är _LeCun_ initialisering, som oftast används med SELU men även just nätverk där antalet noder i alla lager är detsamma.\n",
    "\n",
    "<img src=\"../Data/active_init.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fc19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# av historiska skäl fungerar initialisering lite konstigt i Torch\n",
    "# bäst är att göra det explicit:\n",
    "\n",
    "layer = nn.Linear(40,10)\n",
    "\n",
    "## He-initialisation\n",
    "nn.init.kaiming_uniform_(layer.weight)\n",
    "nn.init.zeros_(layer.bias) # bias kan lika gärna börja på noll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf6d8c",
   "metadata": {},
   "source": [
    "### Aktiveringsfunktioner\n",
    "\n",
    "Så vilken aktiveringsfunktion skall man välja?\n",
    "\n",
    "I praktiken är ReLU väldigt snabb att beräkna och mättas inte för positiva värden (har nollskild gradient), men den har ett problem som kallas \"döende ReLU\" -- under träningen kan dessa aktiveringsfunktioner hamna i ett läga då de alltid får negativ input och alltså alltid ger tillbaka 0. Därefter är den noden permanent 0, och alltså \"död\". \n",
    "\n",
    "En lösning är LeakyReLU:\n",
    "<img src=\"../Data/LeakReLU.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458318bb",
   "metadata": {},
   "source": [
    "Även en variant där vinkeln är en parameter, PReLU, finns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(50, 40),\n",
    "    nn.LeakyReLU(negative_slope=alpha)    \n",
    ")\n",
    "nn.init.kaiming_uniform_(model[0].weight, alpha, nonlinearity=\"leaky_relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368a251",
   "metadata": {},
   "source": [
    "#### GELU, Swish, SwiGLU, Mish, RELU²\n",
    "\n",
    "<img src=\"../Data/GELU.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38c34f",
   "metadata": {},
   "source": [
    "GELU, Mish och Swish finns som <code>nn.GELU</code>, <code>nn.Mish</code> och <code>nn.SiLU</code>\n",
    "\n",
    "Se boken kapitel 11 för detaljer och hur man implementerar de andra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179ff95",
   "metadata": {},
   "source": [
    "Swish och ReLU i praktiken vanligast, men tanh är viktig för LLMer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e1412",
   "metadata": {},
   "source": [
    "## Normalisering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
